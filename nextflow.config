/*
========================================================================================
    nf_irods_to_lustre
========================================================================================
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

workDir =    "${projectDir}/work"
tmpDir =     "${projectDir}/tmp"

params {
  outdir =    "${projectDir}/results"
  reportdir = "${projectDir}/reports"
  tracedir = "${projectDir}/traceDir"
  
      run_mode = "study_id" // you must choose either "study_id" or "csv_samples_id" or "google_spreadsheet".

    // mode "study_id" will list/iget all samples from an input list of Irods study_id(s).
    // mode "csv_samples_id" will iget all samples listed in input tsv file.
    // mode "google_spreadsheet" will iget all samples listed in input google spreadsheet column.

    // if you selected mode "study_id", define these:
    study_id_mode {
	run_imeta_study = true // whether to run task to list all samples and cram from study ids
	input_studies = ['6575'] // list of study_ids to pull, can be more than one (seperated by commas).
    }
    
    // if you selected mode "csv_samples_id", define these:
    csv_samples_id_mode {
	// will iget samples listed one-per-line in input file "samples.tsv":
	// it needs to be tab-separated and have a column name "sanger_sample_id" that matches Irods samples.
	input_samples_csv = "${projectDir}/../../inputs/samples_leo_35464.csv"
	input_samples_csv_column = "sanger_sample_id" // column for Irods sample IDs to search.
    }
    
    // if you selected mode "google_spreadsheet", define these:
    google_spreadsheet_mode {
	// will iget samples listed one-per-line in google spreadsheet:
	// mercury pipeline user must have been configured access to that spreadsheet.
	// it needs to have a column name "sanger_sample_id" that matches Irods samples.
	run_gsheet_to_csv = true // whether to run task to use google API and Service Account to convert google Spreadsheet with list of Irods samples IDs to local csv file.
	input_gsheet_name = "Submission_Data_Pilot_UKB" // name of google sheet, service account below must be granted access to that spreadsheet.
	input_google_creds = "google_api_credentials.json" // file path to service account credentials json, must have been granted access to spreadsheet
	output_csv_name = "Submission_Data_Pilot_UKB.csv" // name of sheet table converted to csv
	input_gsheet_column = "SANGER SAMPLE ID" // column for Irods sample IDs to search.
	run_join_gsheet_metadata = true // whether to run task to
	                                // combine all samples tables (google spreadsheet, irods + cellranger metadata, cellranger /lustre file paths),
	                                //   by joining on common sample column:
	                                // the resulting combined tables can be fed directly as input to the Vireo deconvolution pipeline or QC pipeline.
    }

    // input parameters common to all input modes:
    run_imeta_samples = true // whether to run task to list all samples and cram from provided list of samples IDs from csv input table (created from google Spreadsheet).
    run_imeta_study_cellranger = true // whether to run task to list all cellranger irods objects 
    run_iget_study_cellranger = true // whether to run task to iget all samples cellranger irods objects
    run_iget_study_cram = false // whether to run task to iget all samples cram files
    run_crams_to_fastq = false // whether to run task to merge and convert crams of each sample to fastq
    run_metadata_visualisation = true //whether to visualise the fetched rellranger metadata.
    crams_to_fastq_min_reads = "1000" // minimum number of reads in merged cram file to try and convert to fastq.gz 
    copy_mode = "rellink" // choose "rellink", "symlink", "move" or "copy" to stage in crams and cellranger data from work dir into results dir

    // the following are for one-off tasks run after workflow completion to clean-up work dir:
    on_complete_uncache_irods_search = false // will remove work dir (effectively un-caching) of Irods search tasks that need to be rerun on next NF run even if completed successfully.
    on_complete_remove_workdir_failed_tasks = false // will remove work dirs of failed tasks (.exitcode file not 0)
    // TODO: on_complete_remove_workdir_notsymlinked_in_results = false // will remove work dirs of tasks that are not symlinked anywhere in the results dir. This might uncache tasks.. use carefully..


  singularity_use_pre_cached_images = true

  //Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (~/.nextflow/config) then you don't need to specify this on the command line for every run.
  email_on_complete = ""
  //An email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully.
  email_on_fail = ""
}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

profiles {
    debug { process.beforeScript = 'echo $HOSTNAME' }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    lsf {
        includeConfig 'conf/lsf_base.conf'
        includeConfig 'conf/lsf_tasks.conf'
        singularity{
            cacheDir = '/lustre/scratch118/humgen/resources/containers/'
        }
    }
    lsf_tower{
        includeConfig 'conf/lsf_base.conf'
        includeConfig 'conf/lsf_tasks.conf'
    }
    test_data  { includeConfig 'conf/test_data.conf'}
}
// Export these variables to prevent local Python/R libraries from conflicting with those in the container
env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')

timeline {
    enabled = false
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = false
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = false
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = false
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.svg"
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
